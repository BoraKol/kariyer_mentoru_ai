# ğŸ§  Kariyer Mentor AsistanÄ±  
**LLM + Streamlit + FastAPI + Docker Compose Entegrasyonlu UÃ§tan Uca Proje**

Kariyer Mentor AsistanÄ±, kullanÄ±cÄ±larÄ±n CVâ€™lerini ve baÅŸvurmak istedikleri iÅŸ ilanlarÄ±nÄ± karÅŸÄ±laÅŸtÄ±rarak gÃ¼Ã§lÃ¼ ve zayÄ±f yÃ¶nlerini analiz eden, kiÅŸiselleÅŸtirilmiÅŸ Ã¶neriler sunan bir **LLM tabanlÄ± deÄŸerlendirme sistemi**dir.


## ğŸš€ Ã–zellikler

- ğŸ“„ PDF formatÄ±nda CV yÃ¼kleme desteÄŸi  
- ğŸ’¼ Ä°ÅŸ ilanÄ± metnini girerek detaylÄ± uyumluluk analizi  
- ğŸ§  GeliÅŸmiÅŸ LLM modeli ile gÃ¼Ã§lÃ¼-zayÄ±f yÃ¶n analizi ve Ã¶neriler  
- âš™ï¸ Streamlit tabanlÄ± frontend arayÃ¼z  
- ğŸŒ Render Ã¼zerinde Ã§alÄ±ÅŸan FastAPI tabanlÄ± backend (model API servisi)  
- ğŸ³ Docker destekli frontend mimarisi  
- ğŸŒ Tam TÃ¼rkÃ§e destekli akÄ±llÄ± deÄŸerlendirme sÃ¼reci  

---

## ğŸ§© Proje Mimarisi

Proje iki ana bileÅŸenden oluÅŸur:  
**Frontend (Streamlit)** ve **Backend (FastAPI)**.  

> Backend Render ortamÄ±nda deploy edilmiÅŸtir.  
> Frontend, Docker Compose ile lokal olarak Ã§alÄ±ÅŸtÄ±rÄ±ldÄ±ÄŸÄ±nda Renderâ€™daki backendâ€™e baÄŸlanÄ±r.

---

``` markdown
ğŸ“ kariyer_mentoru_ai/
â”‚
â”œâ”€â”€ .env
â”œâ”€â”€ .gitignore
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ README.md
â”‚
â”œâ”€â”€ backend/ # Render ortamÄ±na deploy edilen API
â”‚   â”œâ”€â”€ app.py
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ requirements.txt
â”‚
â””â”€â”€ frontend/ # Lokal veya Docker Ã¼zerinden Ã§alÄ±ÅŸan Streamlit arayÃ¼zÃ¼
    â”œâ”€â”€ main.py
    â”œâ”€â”€ Dockerfile
    â””â”€â”€ requirements.txt

```

---

## âš™ï¸ Ortam DeÄŸiÅŸkenleri

KÃ¶k dizindeki `.env` dosyasÄ±nda ÅŸu bilgileri tanÄ±mlayÄ±n:

``` bash
BACKEND_URL=https://your-render-backend.onrender.com
MODEL_PROVIDER_API_KEY=your_api_key_here
```

> Bu sayede frontend, Renderâ€™daki backendâ€™e otomatik olarak baÄŸlanÄ±r.

---
 
## ğŸ³ Docker Compose ile Ã‡alÄ±ÅŸtÄ±rma

Backend zaten Render Ã¼zerinde aktif olduÄŸundan, sadece frontendâ€™i Docker Ã¼zerinden baÅŸlatmanÄ±z yeterlidir:

``` bash
docker compose up --build
```

Komut tamamlandÄ±ÄŸÄ±nda Streamlit arayÃ¼zÃ¼ ÅŸu adreste Ã§alÄ±ÅŸacaktÄ±r:
ğŸ‘‰ http://localhost:8501

---

## ğŸŒ Servis EriÅŸimleri

| Servis | Adres |
|--------|-------|
| **Frontend (Streamlit)** | [http://localhost:8501](http://localhost:8501) |
| **Backend (Render)** | [https://your-render-backend.onrender.com](https://your-render-backend.onrender.com) |
| **Backend Docs (Swagger UI)** | [https://your-render-backend.onrender.com/docs](https://your-render-backend.onrender.com/docs) |

---

## ğŸ§  Model Entegrasyonu

Backend, `app.py` iÃ§inde yapÄ±landÄ±rÄ±lmÄ±ÅŸ LLM APIâ€™sine baÄŸlanarak kullanÄ±cÄ± giriÅŸlerini iÅŸler.
KullanÄ±lan model: **Qwen3, DeepSeek-R1, veya Llama4-Maverick-Instruct** gibi geliÅŸmiÅŸ aÃ§Ä±k kaynak modellerden biri olabilir.

Model seÃ§imi `.env` dosyasÄ±ndaki yapÄ±landÄ±rmaya gÃ¶re deÄŸiÅŸtirilebilir.

---

## ğŸ§ª Lokal GeliÅŸtirme Modu
Backendâ€™i Lokal Ã‡alÄ±ÅŸtÄ±rma (Renderâ€™a deploy Ã¶ncesi test iÃ§in)

``` bash
cd backend
pip install -r requirements.txt
uvicorn app:app --reload --port 8000
```
Frontendâ€™i Lokal Ã‡alÄ±ÅŸtÄ±rma

``` bash
cd frontend
pip install -r requirements.txt
streamlit run main.py
```

---

## â˜ï¸ Render Ãœzerinde Backend Deploy AdÄ±mlarÄ±

1. Render.com hesabÄ±nÄ±za giriÅŸ yapÄ±n.
2. Yeni bir Web Service oluÅŸturun.
3. Kaynak olarak backend/ klasÃ¶rÃ¼nÃ¼ iÃ§eren GitHub repoâ€™sunu seÃ§in.
4. Environment: Docker
5. Start Command:
    ``` bash
    sh -c "uvicorn app:app --host 0.0.0.0 --port ${PORT:-10000}"
    ```

6. Deploy tamamlandÄ±ÄŸÄ±nda size bir https://<app-name>.onrender.com URLâ€™si verilir.
7. Bu URLâ€™yi `.env` dosyasÄ±ndaki BACKEND_URL deÄŸerine yazÄ±n.

---

## ğŸ§± KullanÄ±lan Teknolojiler

* Python 3.13+
* FastAPI â€“ Backend API servisi
* Streamlit â€“ KullanÄ±cÄ± arayÃ¼zÃ¼
* Docker â€“ Frontend konteynerizasyonu
* Render â€“ Backend deploy platformu
* Fireworks / HuggingFace / Together API â€“ LLM saÄŸlayÄ±cÄ± entegrasyonlarÄ±
* LangChain â€“ PDF yÃ¼kleme ve metin iÅŸleme desteÄŸi

---

## ğŸ KatkÄ± ve GeliÅŸtirme

KatkÄ±da bulunmak iÃ§in yeni bir branch oluÅŸturun ve pull request gÃ¶nderin.
Yeni model veya analiz Ã§Ä±ktÄ±sÄ± eklemek isterseniz `backend/app.py` iÃ§indeki generate_feedback() fonksiyonunu dÃ¼zenleyebilirsiniz.

---

## ğŸ¬ CanlÄ± Demo

Projeyi Deneyin:
* ğŸ‘‰ [Demo](https://kariyer-mentoru-ai.streamlit.app/)

---


